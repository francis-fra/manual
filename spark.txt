#---------------------------------------------------------
# Preparation
#---------------------------------------------------------
yum install git

# configure git
git config --global user.name "fra"
git config --global user.email "fcnchan@yahoo.com"

# config file location:
~/.gitconfig

# check current config
git config --list

# user.name=fra
# user.email=fcnchan@yahoo.com
# core.excludesfile=~/.gitignore_global

#---------------------------------------------------------
# install scala
#---------------------------------------------------------
# download scala tgz file
tar xvf scala-2.11.7.tgz
# move to usr local
sudo mv scala-2.11.7 /usr/lib/

# either rename or create a soft link
#mv /usr/lib/scala-2.11.7 /usr/lib/scala
#chown hduser:hdgrp /user/lib/scala
# create link (optional??)
sudo ln -s /usr/lib/scala-2.11.7 /usr/lib/scala

# edit .bashrc
export SCALA_HOME=/usr/lib/scala/
export PATH=$SCALA_HOME/bin:$PATH

# check
scala -version

#---------------------------------------------------------
# install sbt (optional)
#---------------------------------------------------------
curl https://bintray.com/sbt/rpm/rpm > bintray-sbt-rpm.repo
sudo mv bintray-sbt-rpm.repo /etc/yum.repos.d/
sudo yum install sbt


#---------------------------------------------------------
# install spark
#---------------------------------------------------------
# Method 1: download prebuilt spark
# https://spark.apache.org/downloads.html
# Method 2: command line download
wget http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0-bin-hadoop2.6.tgz

# extract codes
tar xvf spark-1.6.0-bin-hadoop2.6.tgz
sudo mv spark-1.6.0-bin-hadoop2.6 /usr/lib/

# create a link to the latest spark (optional)
ln -s /usr/lib/spark-1.6.0-bin-hadoop2.6 /usr/lib/spark


# change the ownership of the folder and its element
sudo chown -R hduser:hdgrp /usr/local/spark-1.6.0-bin-hadoop2.6

# setup firewall??
#firewall-cmd --permanent --zone=public --add-port=6066/tcp
#firewall-cmd --permanent --zone=public --add-port=7077/tcp
#firewall-cmd --permanent --zone=public --add-port=8080-8081/tcp
#firewall-cmd --reload

#----------------------------------------------------------------
# edit .bashrc
#----------------------------------------------------------------
export SPARK_HOME=/usr/lib/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

#export PATH=$PATH:$JAVA_HOME/bin  
#export PATH=$PATH:$SBT_HOME/bin
#export SBT_HOME=/usr/share/sbt-launcher-packaging/bin/sbt-launch.jar  

#---------------------------------------------------------
# set up spark
#---------------------------------------------------------
# create log and pid directories
sudo mkdir /var/log/spark
sudo chown hduser:hdgrp /var/log/spark
sudo -u hduser mkdir $SPARK_HOME/run

# optional
#export SPARK_EXAMPLES_JAR=/usr/local/spark-0.9.1-bin-hadoop2/examples/target/scala-2.10/spark-examples_2.10-assembly-0.9.1.jar


#----------------------------------------------------------------
# spark-env.sh
#----------------------------------------------------------------
cd /usr/lib/spark/conf/
cp spark-env.sh.template spark-env.sh

# edit $SPARK_HOME/conf/spark-env.sh
export SPARK_MASTER_IP=127.0.0.1 
export SPARK_WORKER_CORES=1 
export SPARK_WORKER_MEMORY=500mb 
export SPARK_WORKER_INSTANCES=2
export SPARK_LOG_DIR=/var/log/spark
export SPARK_PID_DIR=${SPARK_HOME}/run

#export HADOOP_HOME=${HADOOP_HOME:-/usr/hdp/current/hadoop-client}
#export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/usr/hdp/current/hadoop-client/conf}
#export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
#export SPARK_SUBMIT_OPTIONS="--jars ${SPARK_HOME}/lib/spark-csv_2.11-1.4.0.jar"

#export SPARK_JAVA_OPTS=-Dspark.driver.port=53411
#HADOOP_CONF_DIR=$HADOOP_HOME/conf

#----------------------------------------------------------------
# conf/log4j.properties
#----------------------------------------------------------------
# to change the verbosity level
cp conf/log4j.properties.template conf/log4j.properties
# change the line
log4j.rootCategory=INFO, console
# to this
log4j.rootCategory=ERROR, console

#----------------------------------------------------------------
# Python
#----------------------------------------------------------------
# add to .bashrc:
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

sudo pip install numpy scipy matplotlib ipython jupyter pandas sympy nose

#----------------------------------------------------------------
# edit spark-defaults.conf (optional)
#----------------------------------------------------------------
# edit $SPARK_HOME/conf/spark-defaults.conf
spark.master            spark://master.localhost:7077
spark.serializer        org.apache.spark.serializer.KryoSerializer

# edit slaves config
sudo cp /usr/local/spark/conf/slaves.template /usr/local/spark/conf/slaves














#----------------------------------------------------------------
# Multi Node only
#----------------------------------------------------------------
# edit /etc/hosts
# lists the slave nodes
127.0.0.0.1 localhost
192.168.1.10 spark.slave01.com
192.168.1.11 spark.slave02.com


# edit /usr/local/spark/conf/slaves
# Append hostnames of all the salve nodes in $SPARK_HOME/conf/slaves file
# e.g.
master.backtobazics.com
slave1.backtobazics.com

# do the same on the slave machine


#---------------------------------------------------------
# Starting spark
#---------------------------------------------------------
# start up
/opt/spark-latest/sbin/start-master.sh
# You can monitor Spark using Web GUI
http://192.168.1.8:4040
#http://192.168.1.xx:8080

# spark shell
spark-shell
scala> val textFile = sc.textFile(“README.md”)
scala> textFile.count()

# testing
#run-example org.apache.spark.examples.SparkPi local
cd $SPARK_HOME/bin
./run-example SparkPi 10

# spark python
pyspark

Spark Cluster
http://spark.apache.org/docs/latest/spark-standalone.html


# Master Node IP
#http://spark.master.com:8080

# create home directory for spark user in HDFS??
sudo -u hdfs hadoop fs -mkdir -p /user/hduser
sudo -u hdfs hadoop fs -chown -R hduser:hdgrp /user/spark

# Hive config??
sudo -u hive hadoop fs -mkdir /user/hive/warehouse
sudo -u hdfs hadoop fs -chmod -R 777 /user/hive/warehouse



