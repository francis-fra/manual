#------------------------------------------------------------
# Hadoop / Map Reduce
#------------------------------------------------------------
# edit bashrc:
export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar

# to compile map reduce files
hadoop com.sun.tools.javac.Main <java_src_files>

# create jar file
jar cf <output.jar> <compiled.class>

# start hadoop
start-dfs.sh
start-yarn.sh

# start spark daemon
$SPARK_HOME/sbin/start-all.sh

#------------------------------------------------------------
# running wordcount on hadoop

# compile
hadoop com.sun.tools.javac.Main WordCount.java WordMapper.java SumReducer.java
# pack into jar file
jar cf wc.jar *.class
# copy source file to hdfs
hdfs dfs -copyFromLocal shakespeare.txt /user/fra/shakespeare.txt
# submit map reduce job
hadoop jar wc.jar WordCount shakespeare.txt wordcounts

# python map reduce
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -input /user/fra/flights.csv \
  -output average_delay \
  -mapper mapper.py \
  -reducer reducer.py \
  -file mapper.py \
  -file reducer.py

#------------------------------------------------------------
# Emacs .emacs file
#------------------------------------------------------------
;; BASIC CUSTOMIZATION
;; --------------------------------------

(setq inhibit-startup-message t) ;; hide the startup message
(global-linum-mode t) ;; enable line numbers globally
(column-number-mode t) ;; show column numbers in the stats bar
(require 'ido)
(ido-mode t)

; don't show the menu bar
(menu-bar-mode nil)
; don't show the tool bar
(require 'tool-bar)
(tool-bar-mode nil)

; always use spaces, not tabs, when indenting
(setq indent-tabs-mode nil)


# highlight current line
# download el file and save it to .emacs.d/hightlight-current-line

; highlight the current line
(require 'highlight-current-line)
(global-hl-line-mode t)
(setq highlight-current-line-globally t)
(setq highlight-current-line-high-faces nil)
(setq highlight-current-line-whole-line nil)
(setq hl-line-face (quote highlight))

# elpy
# prerequisite
pip3 install jedi flake8 autopep8 yapf rope importmagic --user

# install script:
(require 'package)
(add-to-list 'package-archives
             '("melpa-stable" . "https://stable.melpa.org/packages/"))

# at minibuf
M-x package-refresh-contents
M-x package-install RET elpy RET
M-x package-install RET ein RET
M-x package-install RET haskell-mode RET

# edit .emacs
(package-initialize)
(elpy-enable)

# set python3
(setq python-shell-interpreter "python3"
      python-shell-interpreter-args "-i")

# to check elpy config
(elpy-config)

# install packages:
(setq package-archives
  '(("gnu" . "http://elpa.gnu.org/packages/")
    ("marmalade" . "https://marmalade-repo.org/packages/")
    ("melpa" . "http://melpa.milkbox.net/packages/")))

;; ein
(require 'ein)
(require 'ein-loaddefs)
(require 'ein-notebook)


;; ESS
# download and extract zip file
(add-to-list 'load-path "/path/to/ESS/lisp/")
(load "ess-site")

(require 'package)
(add-to-list 'package-archives
             '("melpa-stable" . "https://stable.melpa.org/packages/") t)

;; package archives
(custom-set-variables
 ;; custom-set-variables was added by Custom.
 ;; If you edit it by hand, you could mess it up, so be careful.
 ;; Your init file should contain only one such instance.
 ;; If there is more than one, they won't work right.
 '(package-archives
   (quote
    (("gnu" . "http://elpa.gnu.org/packages/")
     ("melpa-stable" . "http://stable.melpa.org/packages/")))))

#------------------------------------------------------------
# to start jupyter:
M-x ein:jupyter-server-start
# to stop:
M-x ein:jupyter-server-stop


#------------------------------------------------------------
# Toree
#------------------------------------------------------------
# start spark session
import org.apache.spark.sql.SparkSession
val spark = SparkSession.
        builder().
        getOrCreate()	
import spark.implicits._


# magic
%AddJar file:///usr/local/lib/java/postgresql-42.2.1.jar

# dependencies
%AddDeps au.com.bytecode opencsv 2.4
import au.com.bytecode.opencsv.CSVReader

%AddDeps org.scalanlp breeze_2.11 0.13.2
%AddDeps org.scalanlp breeze-viz_2.11 0.13.2
%AddDeps org.scalanlp breeze-natives_2.11 0.13.2

#------------------------------------------------------------
# jupyter-scala
#------------------------------------------------------------
# magic

# dependencies
import $ivy.`com.typesafe.slick::slick:3.2.1`

#import $ivy.`org.vegas-viz::vegas:0.3.12`

import $ivy.`org.scalanlp::breeze-natives:0.13.2`
import $ivy.`org.scalanlp::breeze-viz:0.13.2`
import $ivy.`org.scalanlp::breeze:0.13.2`

#------------------------------------------------------------
# Postgresql
#------------------------------------------------------------
# login as user postgres:
sudo -u postgres bash

# login as postgres and start psql client
sudo -u postgres psql postgres

# start postgres client
v# default database is the same as username
psql
psql -U <username> -d <database>

# e.g.
# psql -U fra -d ucidb


#--------------------------------------------------------------------
# admin
#--------------------------------------------------------------------
# create user login (at bash)
createuser --interactive fra

# create new user with password
createuser -P fra
# alternatively,
postgres=# CREATE USER yourname WITH SUPERUSER PASSWORD 'yourpassword';

# create new database
createdb ucidb
# alternatively,
<user>=# CREATE DATABASE <dbname> WITH OWNER = <ownerName>;

# grant permission to create db
<user>=# ALTER USE <user> CREATEDB;
# grant permission
<user>=# GRANT <privilege> TO <user>;

#--------------------------------------------------------------------
# commands:
#--------------------------------------------------------------------
# list all tables
\dt
# connect to database <db>
\c db
# show schema
\d+ <tablename>

# list all users:
<user>=# \du

# set user password
<user>=# \password <user>

# run query at terminal, e.g.
psql -d ucidb -c "select * from abalone limit 10"


--------------------------------------------------------------------
# import csv file to postgressql database
--------------------------------------------------------------------
# create table schema
csvsql abalone.csv > maketable.sql
psql -d <dbname> < maketable.sql

# load data (type inside cli)
\copy <table name> FROM <csv file>  CSV HEADER;

# export table to
\copy customers TO '/tmp/customers.csv' CSV HEADER;

--------------------------------------------------------------------
# extension
--------------------------------------------------------------------
SELECT name, default_version, installed_version, left(comment,30) As comment
FROM pg_available_extensions
WHERE installed_version IS NOT NULL
ORDER BY name;

# to install extension:
# download extension file, then
CREATE EXTENSION <extension>, e.g:
CREATE EXTENSION fuzzystrmatch;


#------------------------------------------------------------
# Docker
#------------------------------------------------------------
# start
docker run -h <host_name> -it <docker_image>

# check image
docker inspect <image_name>

# check running docker
docker ps
docker ps -a

# to remove docker image from local
docker rm <image_name>

# log
docker logs <image_name>

# to save image
docker commit cowsay test/cowsayimage



#------------------------------------------------------------
# CLI Data Science installation
#------------------------------------------------------------
# installation
pip install csvkit
sudo npm install xml2json-command

sudo apt-get install jq

# install json2csv
# no sudo, user based (need golang)
go get github.com/jehiah/json2csv

#------------------------------------------------------------
# Data Science Command Line
#------------------------------------------------------------
cat hello-world | wc -w
< hello-world wc -w

# convert xlsx to csv
in2csv <excel input file> > <csv out file>

# quick look at csv file
cat imdb-250.csv | head -n 10 | csvcut -c Title,Year | csvlook


# download file with curl
curl <url>
# ftp with login credentials
curl -u username:password ftp://host/file

# translate fro lower to upper
echo "hi" | tr '[:lower:]' '[:upper:]'


# TODO: getopts
# word count and sort by frequency
cat shakes.txt | tr '[:upper:]' '[:lower:]' | grep -oE '\w+' | sort | uniq -c | sort -nr | head -n 10


# python / R command line
#! /usr/bin/env python
#! /usr/bin/env Rscript

# header / body / cols

# get the header of a csv file
< tips.csv header

# remove the header
< tips.csv header -d 

# add a header
< tips.csv header -a newheader

# use 'tr' to transform text, but apply to the 'body' only and only to
# the 'day' col only
< tips.csv cols -c day body "tr '[a-z]' '[A-Z]'" | head -n 5 | csvlook

# csvcut
# select all cols except species
< iris.csv csvcut -C species | head -n 5 | csvlook

# csvsql
< iris.csv csvsql --query "SELECT sepal_length, petal_length FROM stdin WHERE sepal_length > 6" | head -n 5 | csvlook

# filtering - csvgrep
# sel size with is 5
csvgrep -c size -r "5" tips.csv | csvlook


# create new cols -- transformation
< names.csv csvsql --query "SELECT id, first_name || ' ' || last_name AS full_name, born FROM stdin" | csvlook

# stack csv (vertical)
csvstack Iris-*.csv 

# horizontal stack of the three files: bills.csv, customers.csv and datetime.csv
paste -d, {bills,customers,datetime}.csv

# inner join
csvjoin -c species iris.csv irismeta.csv | csvcut -c sepal_length,sepal_width,species,usda_id | csvlook

# csvsql
csvsql --query 'SELECT i.sepal_length, i.sepal_width, i.species, m.usda_id FROM iris i JOIN irismeta m ON (i.species = m.species)' iris.csv irismeta.csv | csvlook

#---------------------------------------------------------------------
# Data Science Tools
#---------------------------------------------------------------------
# login name
whoami
# host name
hostname
# current date
date

# data structure
tree 

# which json2csv
home/fra/Project/GoProj/bin/json2csv

# word count
wc -l xxx.csv

# view files
head / less / more / tail


# set an executable python script
# put this at the first line
#!/usr/bin/env python

# set an executable R script
# put this at the first line
#!/usr/bin/env Rscript


# convert excel file to csv file
in2csv xxx.xlsx > xxx.csv

# to quickly view xlsx file
in2csv xxx.xlsx | head | csvcut -c <column names> | csvlook
in2csv xxx.xlsx --sheet <sheet name> | head | csvcut -c <column names> | csvlook

# download from the web
curl -s <url> -o out_file
curl -u username:password ftp://host/file

# word count
cat shakes.txt | tr '[:upper:]' '[:lower:]' | grep -oE '\w+' | sort |
uniq -c | sort -nr | head -n 10

# random sample with specified rate (e.g. 10%)
sample -r 10%

# replace values
tr <original_value> <new_value>
# e.g. echo 'hello world!' | tr ' ' '_'

# delete characters
echo 'hello world!' | tr -d -c '[a-z]'

# munipulate adder
# e.g. to add a header
seq 5 | header -a count

# to extract header
cat tips.csv | header
< tips.csv header

# to delete header
cat tips.csv header -d | head

# apply a certain command only to the body
body
# apply a certain command only to some columns
cols

e.g. change the column 'day' to uppercase
< tips.csv cols -c day body "tr '[a-z]' '[A-Z]'" | head


# do sql query on csv
seq 5 | header -a value | csvsql --query "SELECT SUM(value) AS sum FROM stdin"


# convert XML (HTML) to json
< table.html xml2json > table.json
< table.json jq '.' | head -n 25

# jq : extract and reshape json data
# e.g. take the values and give them labels
< table.json jq -c '.html.body.tr[] | {country: .td[1][],border:'\ '.td[2][], surface: .td[3][]}' > countries.json

# convert json to csv
< countries.json json2csv -p -k border,surface > countries.csv

#---------------------------------------------------------------------------------
# CSV scrub operations
#---------------------------------------------------------------------------------
# extract and reordering
< iris.csv csvcut -c sepal_length,petal_length,sepal_width,petal_width | head | csvlook

# select columns
< tips.csv csvcut -c bill,tip
# select columns and save the results
< tips.csv csvcut -c day,time | tee datetime.csv | head -n 3 | csvlook
< tips.csv csvcut -c sex,smoker,size | tee customers.csv | head -n 3 | csvlook

# exclude species
< iris.csv csvcut -C species | head -n 5 | csvlook

# Filtering
# exclude size with values 1 to 4
csvgrep -c size -i -r "[1-4]" tips.csv | csvlook

#---------------------------------------------------------------------------------
# csvsql
#---------------------------------------------------------------------------------
< iris.csv csvsql --query "SELECT sepal_length, petal_length, sepal_width, petal_width FROM stdin" | head -n 5 | csvlook

< tips.csv csvsql --query "SELECT * FROM stdin WHERE bill > 40 AND day LIKE '%S%'" | csvlook

#---------------------------------------------------------------------------------
# Explore Data
#---------------------------------------------------------------------------------
# to check the data types
# e.g.
csvsql iris.csv


# count num unique values
cat data/iris.csv | csvcut -c species | body "uniq | wc -l"

# show number of unique values in each column
csvstat data/iris.csv --unique

# treat as categorical value if num of unique values is far less than
#  the number of rows

# all statistics
csvstat data/iris.csv

# stat for some selected columns only
csvstat data/iris.csv -c 2,14

# explore data using R
data/iris.csv Rio -e 'summary(df)'


# Create visualization
# examples:
< data/tips.csv Rio -ge 'g+geom_histogram(aes(bill))' | display

< data/tips.csv Rio -ge 'g+geom_density(aes(tip / bill * 100, fill=sex), alpha=0.3) + xlab("percent")' | display

#---------------------------------------------------------------------------------
# Parallel Pipeline
#---------------------------------------------------------------------------------
echo "2.1^3" | bc

# command line for loop
for i in {0..100..2}; do echo "$i^2" | bc ; done

# GNU Parallel
seq 5 | parallel --no-notice "echo {}^2 | bc"

# exmaples
zcat *.json.gz | jq -r '.borough' | tr '[A-Z] ' '[a-z]_' | sort | uniq -c

#---------------------------------------------------------------------------------
# Modeling
#---------------------------------------------------------------------------------
# view multiple files

head wine-{red,white}.csv
wc -l wine-{red,white}.csv

# check missing data
csvstat wine-both-clean.csv --nulls

# install tapkee
sudo apt-get install libeigen3-dev
curl -sL https://github.com/lisitsyn/tapkee/archive/master.tar.gz > tapkee-master.tar.gz

# ERROR: eigen3 lib problems
tar -xzf tapkee-master.tar.gz
cd tapkee-master
mkdir build && cd build

#---------------------------------------------------------------------------------
# Python
#---------------------------------------------------------------------------------
# doctest
python -m doctest -v <source_file or txt_file>

# to enable interact in jupyter
pip install ipywidgets
jupyter nbextension enable --py widgetsnbextension

# bokeh server
bokeh serve eda --show --args abalone.csv

#---------------------------------------------------------------------------------
# gcc / g++
#---------------------------------------------------------------------------------
# compilation c++11
g++ -std=c++11 <file_name>


gcc -std=c99 -Wall problem3.c -lm

#---------------------------------------------------------------------
# generate debug info
gcc -g filename.c

# lib path
gcc -I <header folder>
# output executable name
gcc -o <output executable>

# generate object file
gcc -c <src file>

# to get the size of file
size <object file>

# get a list of symbols
objdump --syms <object file>

# disassemble
objdump -d <object file>


#---------------------------------------------------------------------
# to find the location of glibc shared library
ldd <executatable> | grep libc

# init file
~/.gdbinit

# TUI mode
gdb -tui <executable>


#---------------------------------------------------------------------
# gdb debug
objdump -M intel -D a.out \ grep -A20 main.:

# with debug info
gcc -g <filename>

# inside GDB:
info register (i r)

# set intel format
set disassembly-flavor intel


# display assembly code
disassemble <function>
# example:
disass main

# show content
x/i (instruction) var
x/x (hex) var 
x/d (decimal) var
x/b (binary) var
x/o (octal) var
x/s (string) var (pointer)

# examine variables
print &pointer (address)
print pointer (content)

# display value of var
p <varName>

rbp : frame pointer
rip : execution pointer
rsp : stack pointer

#---------------------------------------------------------------------
# check port usage:

netstat -tulpn | grep LISTEN

# write to port
nc -lk 9999

cat data.txt | ncat -l -p 9999

#---------------------------------------------------------------------
# check intstruction (5 lines) at register rip
x/5i $rip

# register rip points to the current instruction

--------------------------------------------------------------------
# javac
--------------------------------------------------------------------
On Unix, we would do this: (in colon)
javac -classpath dir1:dir2:dir3 ...

# check class path
echo $CLASSPATH

# to specify the class out directory
javac -d <class_out_dir>

######  NOTE:
#if specifying -cp in command line, this would override the
#CLASSPATH setting!


--------------------------------------------------------------------
# java
--------------------------------------------------------------------
# use -cp to specific path (if not in the global class path)
# example 1
# current path:
~/FraDir/learn/introcs/out/production/introcs$

# to run:
java -cp ./:/home/fra/FraDir/learn/introcs/stdlib-package.jar exercise.FindMinMax
# where the java class is under exercise/FindMinMax.class
# the package is called exercise

# example 2
# to compile:
javac -cp ../stdlib.jar RandomSeq.java
# to run: (need to add the current path)
java -cp ./:../stdlib.jar RandomSeq 10

--------------------------------------------------------------------
# maven
--------------------------------------------------------------------
# create folder structure:
mkdir -p src/main/java/hello

# create a pom.xml in the same folder as src

# compile
mvn compile

# install
mvn clean install


--------------------------------------------------------------------
# package
--------------------------------------------------------------------
# if the java class is part of a package
# for example in Precedence.java, it begins with the package keyword followed by the structure

package com.operators;

[parent]
	[com]
		[operators]
		
# in this case, run this command at the parent folder 
java com.operators.Precedence


# javadoc
# to create html documentation
Usage: javadoc [options] [packagenames] [sourcefiles] [@files]

--------------------------------------------------------------------
# introcs
--------------------------------------------------------------------
# with classpath set in .bashrc
# run at /home/fra/FraDir/learn/introcs/src
# Newton.java is part of the package introcs
javac introcs/Newton.java
java introcs.Newton 


--------------------------------------------------------------------
# JAR package
--------------------------------------------------------------------
# to view contents a JAR file
jar tf <jar file>

# to extract contents a JAR file
jar xf <jar file>

# to create a JAR file
jar cf <jar file> <input files>

--------------------------------------------------------------------
# Intellij setup
--------------------------------------------------------------------
javac -cp "C:\Users\m038402\Documents\myWork\Codes\algs4-master\src\main\java" .\edu\princeton\cs\algs4\ThreeSum.java

# standford NLP Core
java -cp "*" -Xmx1g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt

java -mx1g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
http://localhost:9000/


# Think in Java
# for additional libraries, e.g. javaassist and xom, place them under a lib folder (e.g. IdeaProjects/ThinkinJava/lib)
# To import the libraries, put the lib folder in the Modeules->dependencies 

# to get rid of java 1.5 warning:
# add to pom.xml

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.1</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                    <encoding>${project.build.sourceEncoding}</encoding>
                </configuration>
            </plugin>
        </plugins>
    </build>

# in settings -> Build, Execution, Deployment -> Compiler -> Java Compiler
# make sure target bytecode version is 1.8

--------------------------------------------------------------------
Intellij project setup
--------------------------------------------------------------------
#  project structure
Ctrl-ALt_Shift-S

# Project:
# Check SDK : oracle
# Project level : default

# SDK
# setup SDK to the desired java version

# Modules
# setup project structure

# Modules -> Dependencies
# add external libraries here

--------------------------------------------------------------------
Intellij scala / spark project setup
--------------------------------------------------------------------
# create new scala project (sbt)

# project structure:

-- files *.sc are scala worksheet files

-- plugins.sbt
# example: sbt-assembly
   addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.6")


-- build.sbt
# example: spark dependencies

name := "LearnSpark"

version := "0.1"

#scalaVersion := "2.12.3"
scalaVersion := "2.11.8"

libraryDependencies ++= Seq(
  "org.apache.spark" % "spark-core_2.11" % "2.2.0",
  "org.apache.spark" % "spark-sql_2.11" % "2.2.0"
)

# to import dependencies
proejct structure -> Module -> Dependencies

# turn off run workshet inthe compiler process
@ Settings -> languages / Frameworks -> scala worksheet

# send command to scala console
# Ctrl-Shift-X

--------------------------------------------------------------------
# scala test
--------------------------------------------------------------------
# add these to build.sbt
libraryDependencies += "org.scalactic" %% "scalactic" % "3.0.5"
libraryDependencies += "org.scalatest" %% "scalatest" % "3.0.5" % "test"

# add this to global - does not work in plugins.sbt!! (e.g.  ~/.sbt/0.13/global.sbt)
resolvers += "Artima Maven Repository" at "http://repo.artima.com/releases"

# add these to plugins.sbt
addSbtPlugin("com.artima.supersafe" % "sbtplugin" % "1.1.3")

# test source files should be stored in src/test/scala

# run test:
sbt test

--------------------------------------------------------------------
# FIXME: emacs scala
--------------------------------------------------------------------
(use-package ensime
  :ensure t
  :pin melpa-stable)

-- sbt.version=0.13.16
libraryDependencies  ++= Seq(
   "org.ensime" % "sbt-ensime" % "2.4.0"
)


--------------------------------------------------------------------
Scala compilation
--------------------------------------------------------------------
# project structure:
src/main/scala/com/fra/*.scala

# to complie:
scalac src/main/scala/com/fra/*.scala

# to execute: (package com.fra)
# at the top of project, run:
src/main/scala$ scala com.fra.main



--------------------------------------------------------------------
Simple sbt / spark project
--------------------------------------------------------------------
# to create project structure:
sbt

./build.sbt
./src
./src/main
./src/main/scala

# create source file: SimpleApp.scala
./src/main/scala/SimpleApp.scala

/* SimpleApp.scala */
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
    val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println(s"Lines with a: $numAs, Lines with b: $numBs")
    spark.stop()
  }
}

# edit build.sbt
name := "Simple Project"
version := "1.0"
scalaVersion := "2.11.8"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.2.0"


# sbt version:
project/build.properties
sbt.version=1.1.0
#sbt.version=0.13.16


# to package jars
# change directory to the project level
sbt package

# spark-submit to run:
spark-submit --class "SimpleApp" --master local[4] target/scala-2.11/simple-project_2.11-1.0.jar 

--------------------------------------------------------------------
# SBT: set up new project
--------------------------------------------------------------------
# Hello world template
sbt new sbt/scala-seed.g8
cd <project folder>

# start sbt shell
sbt
# compile / package / run
sbt [compile|package|run]

# quit shell
exit (Ctrl-D)

# build definition file:
build.sbt

# create doc (at target/scala-xxx/api/)
sbt doc

# run-main: sbt with multiple main
sbt "run-main ai.fra.Foo"

# alternatively, add a line in build.sbt
mainClass in (Compile, run) := Some("ai.fra.Foo")

# log level: build.sbt
set logLevel := Level.Debug | Level.Info | Level.Warning | Level.Error

--------------------------------------------------------------------
# SBT: spark project with giter8
--------------------------------------------------------------------
# giter8 can be called from sbt, e.g.
sbt new eed3si9n/hello.g8

# g8 spark Project Template
sbt new holdenk/sparkProjectTemplate.g8

# update:
-- ./project/plugins.sbt
-- ./project/build.properties
-- build.sbt

# testing
sbt "run inputFile.txt outputFile.txt"
# submit command:
spark-submit \
  --class com.example.sparkgiter.CountingLocalApp \
  ./target/scala-2.11/sparkgiter_2.11-0.0.1.jar \
  ./alice_wonderland.txt ./output  

--------------------------------------------------------------------
# sbt + spark log4j
--------------------------------------------------------------------
# create src/main/resources
# copy log4j.properties to this folder

# add these lines to build.sbt
javaOptions in run ++= Seq("-Dlog4j.configuration=log4j.properties")
fork := true,
showSuccess := false,
logLevel in run := Level.Warn
    
# log4j control in code:
import org.apache.log4j.{Level, Logger, LogManager}
LogManager.getRootLogger.setLevel(Level.OFF)

# alternatively,
val rootLogger = Logger.getRootLogger()
rootLogger.setLevel(Level.ERROR)
LogManager.getRootLogger.setLevel(Level.OFF)

--------------------------------------------------------------------
# sbt + spark build.sbt
--------------------------------------------------------------------
sparkVersion := "2.2.0",
sparkComponents := Seq(),

// additional libraries
libraryDependencies ++= Seq(
"org.apache.spark" %% "spark-streaming" % "2.2.0" % "provided",
"org.apache.spark" %% "spark-sql" % "2.2.0" % "provided",

"org.rogach" %% "scallop" % "3.1.1"
),

// must have
run in Compile := Defaults.runTask(fullClasspath in Compile, mainClass in (Compile, run), runner in (Compile, run)).evaluated,

// suppress log output
fork := true,
showSuccess := false

--------------------------------------------------------------------
# Scala deployment
--------------------------------------------------------------------
# to show multiple main classes
sbt 
show discoveredMainClasses

# package jars 
sbt package

# list
jar tvf target/scala-2.10/basic_2.10-1.0.jar

# run by scala
scala target/scala-2.10/basic_2.10-1.0.jar

# for java to run
java -cp "${CLASSPATH}:${SCALA_HOME}/lib/scala-library.jar:target/scala-2.10/basic_2.10-1.0.jar" foo.bar.baz.Main

# assemble all jar

# add this line to plugin.sbt:
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.5")

# add these two lines to build.sbt
import AssemblyKeys._
assemblySettings

# run 
sbt assembly

# to run jars in scala:
scala target/scala-xxx/xxxx.jar

# to run jars in java, scala-library.jar is required:
java -cp "${CLASSPATH}:${SCALA_HOME}/lib/scala-library.jar:target/scala-xxxx/xxxx.jar" ai.fra.Main

--------------------------------------------------------------------
# Scala jupyter notebook
--------------------------------------------------------------------
# Toree: import jar files
%AddJar file:///usr/local/lib/java/mysql-connector-java-5.1.42-bin.jar

# jupyter-scala: add library dependencies
import $ivy.`org.scalanlp::breeze:0.13.2`


--------------------------------------------------------------------
# Project status:
--------------------------------------------------------------------
ideaProj:

-- scalaTest
scalaVersion: 2.11.8
sbt version : 0.13.16
OK  	    : Yes
spark	    : Yes
setup	    : Intellij

build.sbt:
libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "2.2.0" % "provided",
  "org.apache.spark" %% "spark-streaming" % "2.2.0" % "provided",
  "org.apache.spark" %% "spark-sql" % "2.2.0" % "provided"
)

plugins.sbt:
addSbtPlugin("org.scalastyle" %% "scalastyle-sbt-plugin" % "1.0.0")
resolvers += "sonatype-releases" at "https://oss.sonatype.org/content/repositories/releases/"
resolvers += "Spark Package Main Repo" at "https://dl.bintray.com/spark-packages/maven"
addSbtPlugin("org.spark-packages" % "sbt-spark-package" % "0.2.6")
addSbtPlugin("com.jsuereth" % "sbt-pgp" % "1.1.0")
addSbtPlugin("org.scoverage" % "sbt-scoverage" % "1.5.1")
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.5")



-- LearnSpark
scalaVersion: 2.11.8
sbt version : 0.13.16
OK  	    : No
spark	    : Yes
setup	    : Intellij
error	    :



sparkProj:
-- hello
scalaVersion: 2.12.3
sbt version : 1.1.0
OK  	    : Yes
spark	    : No
setup	    : sbt project
error	    :

-- simple/simpleapp
scalaVersion: 2.11.8
sbt version : 1.0.2
OK  	    : No
spark	    : Yes
setup	    : sbt project
error	    : org.apache.spark.SparkException: A master URL must be set in your configuration


-- sparksbt/spark-pika
scalaVersion: 2.11.8
sbt version : 0.13.16
OK  	    : No
spark	    : Yes
setup	    : github download
error	    : java.lang.NoClassDefFoundError: org/apache/spark/SparkConf
run	    : sbt "run alice_wonderland.txt output"


-- sparksbt/spark-daria
scalaVersion: 
sbt version : 
OK  	    : 
spark	    : 
setup	    : 
error	    : 



-- sparksbt/spark-git
scalaVersion: 2.11.8
sbt version : 0.13.16
OK  	    : Yes
spark	    : Yes
setup	    : giter8
error	    : 
run	    : sbt "run alice_wonderland.txt output"



-- standalone/first
scalaVersion: 2.11.8
sbt version : 0.13.16
OK  	    : Yes
spark	    : Yes
setup	    : sbt project

# to compile:
sbt clean package
# submit command:
spark-submit \
  --class example.first \
  ./target/scala-2.11/hello_2.11-0.1.0-SNAPSHOT.jar


-- standalone/mini-complete-example
scalaVersion: 2.11.8
sbt version : 0.13.16
OK  	    : Yes
spark	    : Yes
setup	    : learn spark download
error	    : 
run	    : sbt "run alice_wonderland.txt output"

# submit command:
spark-submit \
  --class com.oreilly.learningsparkexamples.mini.scala.WordCount \
  ./target/scala-2.11/learning-spark-mini-example_2.11-0.0.1.jar \
  ./alice_wonderland.txt ./output
  
